{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os pacotes usuais.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "import scipy.signal as ss\n",
    "from scipy import optimize\n",
    "from numba import jit\n",
    "\n",
    "# Determina que o pyplot apresente a imagem no bloco executado.\n",
    "%matplotlib inline\n",
    "#\n",
    "%config IPCompleter.greedy=True\n",
    "# Determina uma quantidade máxima de linhas na exibição de um pandas DataFrame\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "# Carrega o módulo do algoritmo de segmentação.\n",
    "import segmentation_algorithm as seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai o titulo dos arquivos de todos os três grupos de pacientes, a fim de preservar\n",
    "# a ordem alfabética em que estão os arquivos.\n",
    "# Guarda os titulos em um arquivo, na sequência lida, para recarregá-los posteriormente \n",
    "# em qualquer computador e na mesma ordem.\n",
    "\n",
    "all_file_titles = os.listdir(\"dados/complete\")\n",
    "hypertensive_file_titles = []\n",
    "normotensive_file_titles = []\n",
    "proband_file_titles = []\n",
    "\n",
    "for i in range(len(all_file_titles)):\n",
    "    if \"htm_\" in all_file_titles[i]:\n",
    "        hypertensive_file_titles.append(all_file_titles[i])\n",
    "    elif \"ntm_\" in all_file_titles[i] or \"ntb\" in all_file_titles[i]:\n",
    "        normotensive_file_titles.append(all_file_titles[i])\n",
    "    elif \"pb_\" in all_file_titles[i]:\n",
    "        proband_file_titles.append(all_file_titles[i])\n",
    "        \n",
    "ht_file_titles = open(\"file_titles_ht.txt\", \"w\")\n",
    "nt_file_titles = open(\"file_titles_nt.txt\", \"w\")\n",
    "pb_file_titles = open(\"file_titles_pb.txt\", \"w\")\n",
    "\n",
    "for i in range(len(hypertensive_file_titles)):\n",
    "    hypertensive_file_titles[i] = 'dados/complete/'+hypertensive_file_titles[i][:-4]\n",
    "    ht_file_titles.write(hypertensive_file_titles[i]+\"\\n\")\n",
    "for i in range(len(normotensive_file_titles)):\n",
    "    normotensive_file_titles[i] = 'dados/complete/'+normotensive_file_titles[i][:-4]\n",
    "    nt_file_titles.write(normotensive_file_titles[i]+\"\\n\")\n",
    "for i in range(len(proband_file_titles)):\n",
    "    proband_file_titles[i] = 'dados/complete/'+proband_file_titles[i][:-4]\n",
    "    pb_file_titles.write(proband_file_titles[i]+\"\\n\")\n",
    "    \n",
    "ht_file_titles.close()\n",
    "nt_file_titles.close()\n",
    "pb_file_titles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os titulos dos arquivos salvos anteriormente\n",
    "\n",
    "ht_file_titles = open(\"file_titles_ht.txt\", \"r\")\n",
    "nt_file_titles = open(\"file_titles_nt.txt\", \"r\")\n",
    "pb_file_titles = open(\"file_titles_pb.txt\", \"r\")\n",
    "hypertensive_file_titles = [x.strip() for x in ht_file_titles.readlines()]\n",
    "normotensive_file_titles = [x.strip() for x in nt_file_titles.readlines()]\n",
    "proband_file_titles      = [x.strip() for x in pb_file_titles.readlines()]\n",
    "ht_file_titles.close(); nt_file_titles.close(); pb_file_titles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Carrega e guarda os dados de todos os pacientes de cada grupo em uma lista de dataframes\n",
    "\n",
    "hypertensive_data = []\n",
    "normotensive_data = []\n",
    "proband_data = []\n",
    "titulos = ['Tempo', 'Serie 0', 'Serie 1', 'Serie 2', 'Serie 3', 'SleepStage']\n",
    "\n",
    "for i in range(len(hypertensive_file_titles)):\n",
    "    brute_data = pd.read_table(hypertensive_file_titles[i]+\".dat\", sep='\\t', names = range(20))\n",
    "    hypertensive_data.append(brute_data[[1, 2, 3, 4, 5, 11]])\n",
    "    hypertensive_data[i].columns = titulos\n",
    "for i in range(len(normotensive_file_titles)):\n",
    "    brute_data = pd.read_table(normotensive_file_titles[i]+\".dat\", sep='\\t', names = range(20))\n",
    "    normotensive_data.append(brute_data[[1, 2, 3, 4, 5, 11]])\n",
    "    normotensive_data[i].columns = titulos\n",
    "for i in range(len(proband_file_titles)):\n",
    "    brute_data = pd.read_table(proband_file_titles[i]+\".dat\", sep='\\t', names = range(20))\n",
    "    proband_data.append(brute_data[[1, 2, 3, 4, 5, 11]])\n",
    "    proband_data[i].columns = titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula os saltos das séries.\n",
    "\n",
    "series = [\"Serie 0\", \"Serie 1\", \"Serie 2\", \"Serie 3\"]\n",
    "grupos = [hypertensive_data, normotensive_data, proband_data]\n",
    "for dados in grupos:\n",
    "    for k in series:\n",
    "        for j in range(len(dados)):\n",
    "            saltos = np.zeros(len(dados[j]))\n",
    "            for i in range(len(dados[j]) - 1):\n",
    "                saltos[i] = dados[j][k].loc[i+1] - dados[j][k].loc[i]\n",
    "\n",
    "            saltos[len(dados[j]) - 1] = np.nan\n",
    "            dados[j][k+\" Saltos\"] = saltos\n",
    "            \n",
    "# Salva os dados com os saltos em outra pasta\n",
    "\n",
    "for i in range(len(hypertensive_data)):\n",
    "    hypertensive_data[i].to_csv(\"dados/dados_brutos_com_saltos/\"+hypertensive_file_titles[i][15:]+\".csv\")\n",
    "for i in range(len(normotensive_data)):\n",
    "    normotensive_data[i].to_csv(\"dados/dados_brutos_com_saltos/\"+normotensive_file_titles[i][15:]+\".csv\")\n",
    "for i in range(len(proband_data)):\n",
    "    proband_data[i].to_csv(\"dados/dados_brutos_com_saltos/\"+proband_file_titles[i][15:]+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aplica a segmentação nos dados (em forma matricial).\n",
    "titulos = ['Tempo', 'Serie 0', 'Serie 1', 'Serie 2', 'Serie 3']\n",
    "\n",
    "# Defini os grupos cujas séries serão segmentadas.\n",
    "group = \"proband\"\n",
    "data = proband_data\n",
    "group_file_titles = proband_file_titles\n",
    "\n",
    "result_dataframes = []\n",
    "for i in range(len(data)):\n",
    "    temp = [pd.DataFrame()]*4\n",
    "    print(\"Subject\", i)\n",
    "    for j in range(4):\n",
    "        print(\"\\n Segmenting\")\n",
    "        temp[j] = seg.segment(data[i][titulos[j+1]].values)\n",
    "    \n",
    "    result_dataframes.append(temp.copy())\n",
    "    print(\"Finished segmentation of the subject\", i,\"data.\")\n",
    "    \n",
    "    \n",
    "# Classificação de todas as segmentações por fases do sono.\n",
    "# A classificação é feita considerando a fase de maior incidência no segmento.\n",
    "for i in range(len(data)):\n",
    "    for j in range(4):\n",
    "        for k in list(result_dataframes[i][j].index):\n",
    "            start = int(result_dataframes[i][j][\"start\"][k])\n",
    "            finish = int(result_dataframes[i][j][\"finish\"][k])\n",
    "            result_dataframes[i][j].loc[k:k, \"SleepStage\"] = int(data[i][\"SleepStage\"][start:finish+1].value_counts().idxmax())    \n",
    "    \n",
    "\n",
    "# guarda os dados em .csv\n",
    "for i in range(len(data)):\n",
    "    for j in range(4):\n",
    "        result_dataframes[i][j].to_csv(\"dados/segmentation_dataframes_\"+group+\"/\"+group_file_titles[i][15:][:-4]+\"_\"+titulos[j+1]+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrega os dataframes dos dados brutos e dos resultados da segmentação\n",
    "\n",
    "text_files = [open(\"file_titles_ht.txt\", \"r\"), open(\"file_titles_nt.txt\", \"r\"), open(\"file_titles_pb.txt\", \"r\")]\n",
    "file_titles = [[x.strip() for x in text_files[0].readlines()],\n",
    "               [x.strip() for x in text_files[1].readlines()],\n",
    "               [x.strip() for x in text_files[2].readlines()]]\n",
    "text_files[0].close(); text_files[1].close(); text_files[2].close()\n",
    "\n",
    "\n",
    "number_of_subjects = [len(file_titles[0]), len(file_titles[1]), len(file_titles[2])]\n",
    "seg_res = [[], [], []]\n",
    "data = [[], [], []]\n",
    "for j in [0, 1, 2]:\n",
    "    for i in range(number_of_subjects[j]):\n",
    "        data[j].append(pd.read_csv(\"dados/dados_brutos_com_saltos/\"+file_titles[j][i][15:]+\".csv\", index_col = 0))\n",
    "\n",
    "folder = [\"dados/segmentation_dataframes_hypertensive/\",\n",
    "          \"dados/segmentation_dataframes_normotensive/\",\n",
    "          \"dados/segmentation_dataframes_proband/\"]\n",
    "series = [\"Serie 0\", \"Serie 1\", \"Serie 2\", \"Serie 3\"]\n",
    "for j in [0, 1, 2]:    \n",
    "    for i in range(number_of_subjects[j]):\n",
    "        temp = [pd.DataFrame()]*4\n",
    "\n",
    "        for k in range(4):\n",
    "            temp[k] = pd.read_csv(folder[j]+file_titles[j][i][15:][:-4]+\"_\"+series[k]+\".csv\").set_index(\"index\")\n",
    "            length = len(temp[k].index)\n",
    "\n",
    "        seg_res[j].append(temp.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A filtragem das séries consiste em percorrer cada segmento, calculando, primeiramente, os saltos nos extremos 0.85 e 0.15% da série (o salto no ponto i corresponde à diferença entre i e i-1), trocando os valores correspondentes para a mediana do segmento, e recalculando a média, por fim.\n",
    "\n",
    "Os saltos são calculados novamente, e o algoritmo de filtragem executado mais uma vez, buscando saltos nos extremos 0.98 e 0.02%.\n",
    "\n",
    "*****\n",
    "\n",
    "Posso me questionar se há a necessidade de aplicar novamente o algoritmo de segmentação, pela possibilidade de que o resultado se altere, particularmente nas séries em que a incidência de outliers era muito grande. Posso pensar que, se a filtragem que fiz foi eficiente, o algoritmo de segmentação é bastante útil para alguma técnica de filtragem em que janelas dentro da série devem ser escolhidas, e não é conveniente filtrar a série inteira de uma vez só.\n",
    "\n",
    "Quais outros mecanismos de filtragem de séries temporais existem?\n",
    "\n",
    "Todos baseados em uma janela móvel que substitui valores pela mediana da janela, análogo ao modo que apliquei. O único que compensa ser testado é o filtro de Hampel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra a série\n",
    "#    Já Filtrada\n",
    "\n",
    "# Definição da função de filtragem.\n",
    "\n",
    "def filtro(qmax):\n",
    "    title = [\"Serie 0\", \"Serie 1\", \"Serie 2\", \"Serie 3\"]\n",
    "    title_jp = [\"Serie 0 Saltos\", \"Serie 1 Saltos\", \"Serie 2 Saltos\", \"Serie 3 Saltos\"]\n",
    "    for i in range(3):\n",
    "        for j in range(len(seg_res[i])):\n",
    "            for k in range(4):\n",
    "                serie = data[i][j][title[k]]\n",
    "                for l in range(len(seg_res[i][j][k])):\n",
    "                    start = int(seg_res[i][j][k].loc[l+1][\"start\"])\n",
    "                    finish = int(seg_res[i][j][k].loc[l+1][\"finish\"])\n",
    "                    sslice = data[i][j][title[k]][start: finish+1]\n",
    "                    sslice_jumps = data[i][j][title_jp[k]][start: finish+1]\n",
    "\n",
    "                    maxpercentile = sslice_jumps.quantile(qmax)\n",
    "                    minpercentile = sslice_jumps.quantile(1-qmax)\n",
    "                    sslice_jumps_id = sslice_jumps[ (sslice_jumps < minpercentile) | (sslice_jumps > maxpercentile)].index\n",
    "                    sslice[sslice_jumps_id] = sslice.median()\n",
    "                    sslice[sslice_jumps_id] = sslice.median()\n",
    "\n",
    "                    seg_res[i][j][k].loc[l+1][\"mean\"] = sslice.mean()\n",
    "                    \n",
    "\n",
    "# Definição da função de cálculo dos saltos\n",
    "def calc_saltos():\n",
    "    series = [\"Serie 0\", \"Serie 1\", \"Serie 2\", \"Serie 3\"]\n",
    "    for dados in data:\n",
    "        for k in series:\n",
    "            for j in range(len(dados)):\n",
    "                saltos = np.zeros(len(dados[j]))\n",
    "                for i in range(len(dados[j]) - 1):\n",
    "                    saltos[i] = dados[j][k].loc[i+1] - dados[j][k].loc[i]\n",
    "\n",
    "                saltos[len(dados[j]) - 1] = np.nan\n",
    "                dados[j][k+\" Saltos\"] = saltos\n",
    "\n",
    "# Primeira passagem\n",
    "filtro(0.85)\n",
    "calc_saltos()\n",
    "# Segunda passagem\n",
    "filtro(0.98)\n",
    "calc_saltos()\n",
    "\n",
    "# Salva os dados.\n",
    "\n",
    "for i in range(len(data[0])):\n",
    "    data[0][i].to_csv(\"dados/dados_brutos_com_saltos/\"+hypertensive_file_titles[i][15:]+\".csv\")\n",
    "for i in range(len(data[1])):\n",
    "    data[1][i].to_csv(\"dados/dados_brutos_com_saltos/\"+normotensive_file_titles[i][15:]+\".csv\")\n",
    "for i in range(len(data[2])):\n",
    "    data[2][i].to_csv(\"dados/dados_brutos_com_saltos/\"+proband_file_titles[i][15:]+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de aplicação do filtro de Hampel\n",
    "\n",
    "# Definição da função de Hampel\n",
    "# Créditos: Eduardo Osorio,\n",
    "# https://stackoverflow.com/questions/46819260/filtering-outliers-how-to-make-median-based-hampel-function-faster\n",
    "def hampel(vals_orig, k=7, t0=3):\n",
    "    '''\n",
    "    vals: pandas series of values from which to remove outliers\n",
    "    k: size of window (including the sample; 7 is equal to 3 on either side of value)\n",
    "    '''\n",
    "\n",
    "    #Make copy so original not edited\n",
    "    vals = vals_orig.copy()\n",
    "\n",
    "    #Hampel Filter\n",
    "    L = 1.4826\n",
    "    rolling_median = vals.rolling(window=k, center=True).median()\n",
    "    MAD = lambda x: np.median(np.abs(x - np.median(x)))\n",
    "    rolling_MAD = vals.rolling(window=k, center=True).apply(MAD)\n",
    "    threshold = t0 * L * rolling_MAD\n",
    "    difference = np.abs(vals - rolling_median)\n",
    "\n",
    "    '''\n",
    "    Perhaps a condition should be added here in the case that the threshold value\n",
    "    is 0.0; maybe do not mark as outlier. MAD may be 0.0 without the original values\n",
    "    being equal. See differences between MAD vs SDV.\n",
    "    '''\n",
    "\n",
    "    outlier_idx = difference > threshold\n",
    "    vals[outlier_idx] = np.nan\n",
    "    return(vals)\n",
    "\n",
    "serie_t = normotensive_data[6][\"Serie 2\"]\n",
    "tempo = normotensive_data[6][\"Tempo\"]\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 1)\n",
    "fig.set_size_inches(40, 45)\n",
    "\n",
    "for i in range(10):\n",
    "    serie_t = hampel(serie_t)\n",
    "\n",
    "axs[0].plot(normotensive_data[6][\"Tempo\"], normotensive_data[6][\"Serie 2\"])\n",
    "axs[0].set_title(\"Série original\")\n",
    "\n",
    "axs[1].plot(data[1][6][\"Tempo\"], data[1][6][\"Serie 2\"])\n",
    "axs[1].set_title(\"Filtrada após segmentação\")\n",
    "\n",
    "axs[2].plot(tempo, serie_t)\n",
    "axs[2].set_title(\"Filtrada pela função de Hampel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta a proporção de fases em cada segmento\n",
    "#   Já computado.\n",
    "\n",
    "s_titles = [\"SBP\", \"DBP\", \"Procure1\", \"Procure2\"]\n",
    "for i in range(3):\n",
    "    for j in range(len(seg_res[i])):\n",
    "        for k in range(4):\n",
    "            seg_size = len(seg_res[i][j][k])\n",
    "            stage_count = pd.DataFrame({\"0_%\": np.zeros(seg_size), \"1_%\": np.zeros(seg_size), \n",
    "                                        \"2_%\": np.zeros(seg_size), \"3_%\": np.zeros(seg_size), \"4_%\": np.zeros(seg_size), \n",
    "                                        \"REM_%\": np.zeros(seg_size), \"66_%\": np.zeros(seg_size)})\n",
    "            stage_count.index += 1\n",
    "            for l in range(seg_size):\n",
    "                seg_start = int(seg_res[i][j][k].loc[l+1][\"start\"])\n",
    "                seg_finish = int(seg_res[i][j][k].loc[l+1][\"finish\"])\n",
    "                seg_slice = data[i][j][seg_start:seg_finish+1]\n",
    "                seg_slice_count = seg_slice[\"SleepStage\"].value_counts()\n",
    "                if 0.0 in seg_slice_count.index:\n",
    "                    stage_count.loc[l+1][\"0_%\"] = np.round(seg_slice_count[0.0]/(seg_finish - seg_start + 1), decimals = 3)\n",
    "                if 1.0 in seg_slice_count.index:\n",
    "                    stage_count.loc[l+1][\"1_%\"] = np.round(seg_slice_count[1.0]/(seg_finish - seg_start + 1), decimals = 3)\n",
    "                if 2.0 in seg_slice_count.index:\n",
    "                    stage_count.loc[l+1][\"2_%\"] = np.round(seg_slice_count[2.0]/(seg_finish - seg_start + 1), decimals = 3)\n",
    "                if 3.0 in seg_slice_count.index:\n",
    "                    stage_count.loc[l+1][\"3_%\"] = np.round(seg_slice_count[3.0]/(seg_finish - seg_start + 1), decimals = 3)\n",
    "                if 4.0 in seg_slice_count.index:\n",
    "                    stage_count.loc[l+1][\"4_%\"] = np.round(seg_slice_count[4.0]/(seg_finish - seg_start + 1), decimals = 3)\n",
    "                if 5.0 in seg_slice_count.index:\n",
    "                    stage_count.loc[l+1][\"REM_%\"] = np.round(seg_slice_count[5.0]/(seg_finish - seg_start + 1), decimals = 3)\n",
    "                if 66.0 in seg_slice_count.index:\n",
    "                    stage_count.loc[l+1][\"66_%\"] = np.round(seg_slice_count[66.0]/(seg_finish - seg_start + 1), decimals = 3)\n",
    "                    \n",
    "            seg_res[i][j][k] = seg_res[i][j][k].join(stage_count)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salva os dataframes dos segmentos\n",
    "\n",
    "grupos = [\"hypertensive\", \"normotensive\", \"proband\"]\n",
    "serie  = [\"Serie 0\", \"Serie 1\", \"Serie 2\", \"Serie 3\"]\n",
    "pasta  = [\"segmentation_dataframes_hypertensive\", \"segmentation_dataframes_normotensive\",\n",
    "          \"segmentation_dataframes_proband\"]\n",
    "for i in range(3):\n",
    "    for j in range(len(file_titles[i])):\n",
    "        title = file_titles[i][j][15:][:-4]\n",
    "        for k in range(4):\n",
    "            seg_res[i][j][k].to_csv(\"dados/\"+pasta[i]+\"/\"+title+\"_\"+serie[k]+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plota as séries\n",
    "\n",
    "# Definição da função para gerar as cores de fundo\n",
    "def axvlines(xs, c, max_val, ax=None, **plot_kwargs):\n",
    "    \"\"\"\n",
    "    Draw vertical lines on plot\n",
    "    :param xs: A scalar, list, or 1D array of horizontal offsets\n",
    "    :param ax: The axis (or none to use gca)\n",
    "    :param plot_kwargs: Keyword arguments to be passed to plot\n",
    "    :return: The plot object corresponding to the lines.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xs = np.array((xs, ) if np.isscalar(xs) else xs, copy=False)\n",
    "    lims = ax.get_ylim()\n",
    "    x_points = np.repeat(xs[:, None], repeats=3, axis=1).flatten()\n",
    "    y_points = np.repeat(np.array(lims + (np.nan, ))[None, :], repeats=len(xs), axis=0).flatten()\n",
    "    plot = ax.plot(x_points, max_val*y_points, scaley = False, **plot_kwargs, color = c)\n",
    "    return plot\n",
    "\n",
    "colors  = [\"gold\", \"lavender\", \"lightsteelblue\", \"thistle\", \"orchid\", \"lightcoral\"]\n",
    "t_serie = [\"Serie 0\", \"Serie 1\", \"Serie 2\", \"Serie 3\"]\n",
    "t_serie_nome = [\"SBP\", \"DBP\", \"BBI-BP (ms)\", \"BBI-KG (ms)\"]\n",
    "t_grupo = [\"Hypertensive\", \"Normotensive\", \"Proband\"]\n",
    "\n",
    "for grupo in [0, 1, 2]:\n",
    "    for individuo in range(len(data[grupo])):\n",
    "        fig, axs = plt.subplots(nrows = 4, ncols = 1)\n",
    "        fig.set_size_inches(30, 25)\n",
    "        fig.subplots_adjust(hspace = 0.3, wspace = 0.5)\n",
    "        \n",
    "        for serie in [0, 1, 2, 3]:\n",
    "            axis = axs[serie]\n",
    "            result = seg_res[grupo][individuo][serie]\n",
    "            series = data[grupo][individuo][t_serie[serie]]\n",
    "            time = data[grupo][individuo][\"Tempo\"]\n",
    "            means = np.zeros(0)\n",
    "            max_val = series.max()\n",
    "            min_val = series.min()\n",
    "\n",
    "            for i in result.index:\n",
    "                size = int(result.loc[i,'finish'] - result.loc[i, 'start'] + 1)\n",
    "                vec = np.full([size], result.loc[i, 'mean'])\n",
    "                means = np.concatenate((means,vec))\n",
    "            \n",
    "            for i in range(6):\n",
    "                axvlines(data[grupo][individuo][data[grupo][individuo][\"SleepStage\"] == float(i)][\"Tempo\"].values, colors[i], max_val, ax = axis)\n",
    "\n",
    "            axis.plot(time, series, color = 'dimgray')\n",
    "            axis.plot(time, means, 'r-', linewidth = 2, drawstyle='steps-post', label='step-post')\n",
    "            axis.set_xticks(range(0, int(time[len(time)-1]), 5000))\n",
    "            axis.set_ylim(int(min_val) - 20, int(max_val))\n",
    "            axis.set_xlim(time[0], time[len(time)-1])\n",
    "            axis.set_xlabel('tempo (ms)', fontsize = 20)\n",
    "            axis.set_ylabel('intervalo entre batimentos (s)', fontsize = 20)\n",
    "            axis.set_title(t_serie_nome[serie], fontsize = 25)\n",
    "        \n",
    "        fig.savefig(\"Plots/Series - Final, Com Cores de Fundo/\"+t_grupo[grupo]+\"/Subject\"+str(individuo)+\".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de fato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula as medianas dos segmentos\n",
    "\n",
    "series = [\"Serie 0\", \"Serie 1\", \"Serie 2\", \"Serie 3\"]\n",
    "for grupo in range(3):\n",
    "    for indiv in range(len(seg_res[grupo])):\n",
    "        for serie in range(len(seg_res[grupo][indiv])):\n",
    "            resultado = seg_res[grupo][indiv][serie]\n",
    "            mediana = np.zeros(len(resultado))\n",
    "            \n",
    "            for index in resultado.index:\n",
    "                start = int(resultado[\"start\"][index])\n",
    "                finish = int(resultado[\"finish\"][index])\n",
    "                \n",
    "                mediana[index - 1] = data[grupo][indiv][series[serie]][start: finish+1].median()\n",
    "            \n",
    "            resultado[\"Median\"] = mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula o tempo de início, fim e duração de cada segmento.\n",
    "\n",
    "for grupo in range(3):\n",
    "    for indiv in range(len(seg_res[grupo])):\n",
    "        for serie in range(len(seg_res[grupo][indiv])):\n",
    "            t_i = np.zeros(len(seg_res[grupo][indiv][serie]))\n",
    "            t_f = np.zeros(len(seg_res[grupo][indiv][serie]))\n",
    "            dt  = np.zeros(len(seg_res[grupo][indiv][serie]))\n",
    "            L   = np.zeros(len(seg_res[grupo][indiv][serie]))\n",
    "            \n",
    "            for index in range(1, len(seg_res[grupo][indiv][serie])+1):\n",
    "                start  = int(seg_res[grupo][indiv][serie][\"start\"][index])\n",
    "                finish = int(seg_res[grupo][indiv][serie][\"finish\"][index])\n",
    "                t_i[index-1] = data[grupo][indiv][\"Tempo\"][start]\n",
    "                t_f[index-1] = data[grupo][indiv][\"Tempo\"][finish]\n",
    "                dt[index-1]  = data[grupo][indiv][\"Tempo\"][finish] - data[grupo][indiv][\"Tempo\"][start]\n",
    "                L[index-1]   = finish - start\n",
    "            \n",
    "            seg_res[grupo][indiv][serie][\"T_i\"] = t_i\n",
    "            seg_res[grupo][indiv][serie][\"T_f\"] = t_f\n",
    "            seg_res[grupo][indiv][serie][\"Dt\"]  = dt\n",
    "            seg_res[grupo][indiv][serie][\"L\"]   = L    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula o tempo de início, fim e duração de cada segmento.\n",
    "\n",
    "for grupo in range(3):\n",
    "    for indiv in range(len(seg_res[grupo])):\n",
    "        for serie in range(len(seg_res[grupo][indiv])):\n",
    "            dt  = np.zeros(len(seg_res[grupo][indiv][serie]))\n",
    "            \n",
    "            for index in range(1, len(seg_res[grupo][indiv][serie])+1):\n",
    "                start  = int(seg_res[grupo][indiv][serie][\"start\"][index])\n",
    "                finish = int(seg_res[grupo][indiv][serie][\"finish\"][index])\n",
    "                dt[index-1]  = finish - start\n",
    "            \n",
    "            seg_res[grupo][indiv][serie][\"L\"]  = dt\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleta os segmentos com fase indeterminada ou com frequência máxima menor que 90%\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(len(seg_res[i])):\n",
    "        for k in range(4):\n",
    "            seg_res[i][j][k] = seg_res[i][j][k].drop(seg_res[i][j][k][seg_res[i][j][k][\"SleepStage\"] == 66.0].index)\n",
    "            for l in seg_res[i][j][k].index:\n",
    "                if max(seg_res[i][j][k].loc[l, \"0_%\":\"66_%\"]) < 0.9:\n",
    "                    seg_res[i][j][k] = seg_res[i][j][k].drop(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleta os segmentos com fase indeterminada ou com frequência máxima menor que 90%\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(len(seg_res[i])):\n",
    "        for k in range(4):\n",
    "            seg_res[i][j][k] = seg_res[i][j][k].drop(seg_res[i][j][k][seg_res[i][j][k][\"SleepStage\"] == 66.0].index)\n",
    "            for l in seg_res[i][j][k].index:\n",
    "                if max(seg_res[i][j][k].loc[l, \"0_%\":\"66_%\"]) < 0.85:\n",
    "                    seg_res[i][j][k] = seg_res[i][j][k].drop(l)\n",
    "\n",
    "# aglutinação das estatísticas dos segmentos de todos os sujeitos para a série SBP\n",
    "\n",
    "medias     = [[], [], []]\n",
    "medianas   = [[], [], []]\n",
    "variancias = [[], [], []]\n",
    "tamanhos   = [[], [], []]\n",
    "t_inicio   = [[], [], []]\n",
    "t_final    = [[], [], []]\n",
    "delta_t    = [[], [], []]\n",
    "\n",
    "for grupo in [0, 1, 2]:\n",
    "    medias_series_temp     = [[], [], [], []]\n",
    "    medianas_series_temp   = [[], [], [], []]\n",
    "    variancias_series_temp = [[], [], [], []]\n",
    "    tamanhos_series_temp   = [[], [], [], []]\n",
    "    t_inicio_series_temp   = [[], [], [], []]\n",
    "    t_final_series_temp    = [[], [], [], []]\n",
    "    delta_t_series_temp    = [[], [], [], []]\n",
    "    \n",
    "    for serie in [0, 1, 2, 3]:\n",
    "        # Isso pode ser simplificado se eu reenumerar as fases\n",
    "        medias_temp     = [[], [], [], [], [], []]\n",
    "        medianas_temp   = [[], [], [], [], [], []]\n",
    "        variancias_temp = [[], [], [], [], [], []]\n",
    "        tamanhos_temp   = [[], [], [], [], [], []]\n",
    "        t_inicio_temp   = [[], [], [], [], [], []]\n",
    "        t_final_temp    = [[], [], [], [], [], []]\n",
    "        delta_t_temp    = [[], [], [], [], [], []]\n",
    "\n",
    "        for i in range(len(data[grupo])): \n",
    "            df = seg_res[grupo][i][serie]  \n",
    "            for j in [0, 1, 2, 3, 4, 5]:\n",
    "                for index in df.where(df[\"SleepStage\"] == j).dropna().index:\n",
    "                    medias_temp[j].append(df[\"mean\"][index])\n",
    "                    medianas_temp[j].append(df[\"Median\"][index])\n",
    "                    variancias_temp[j].append(df[\"variance\"][index])\n",
    "                    tamanhos_temp[j].append(df[\"finish\"][index] - df[\"start\"][index])\n",
    "                    t_inicio_temp[j].append(df[\"T_i\"][index])\n",
    "                    t_final_temp[j].append(df[\"T_f\"][index])\n",
    "                    delta_t_temp[j].append(df[\"Dt\"][index])\n",
    "\n",
    "        # Coloca os valores em escala logaritmica.\n",
    "        for i in range(6):\n",
    "            variancias_temp[i] = np.log(np.asarray(variancias_temp[i]))\n",
    "            tamanhos_temp[i]   = np.log(np.asarray(tamanhos_temp[i]))\n",
    "            medias_temp[i]     = np.log(np.asarray(medias_temp[i]))\n",
    "            medianas_temp[i]   = np.log(np.asarray(medianas_temp[i]))\n",
    "            # transforma as listas restantes em arrays\n",
    "            t_inicio_temp[i]   = np.asarray(t_inicio_temp[i])\n",
    "            t_final_temp[i]    = np.asarray(t_final_temp[i])\n",
    "            delta_t_temp[i]    = np.asarray(delta_t_temp[i])\n",
    "        \n",
    "        medias_series_temp[serie]     = medias_temp\n",
    "        medianas_series_temp[serie]   = medianas_temp\n",
    "        variancias_series_temp[serie] = variancias_temp\n",
    "        tamanhos_series_temp[serie]   = tamanhos_temp\n",
    "        t_inicio_series_temp[serie]   = t_inicio_temp\n",
    "        t_final_series_temp[serie]    = t_final_temp\n",
    "        delta_t_series_temp[serie]    = delta_t_temp\n",
    "    \n",
    "    medias[grupo]       = medias_series_temp\n",
    "    medianas[grupo]     = medianas_series_temp\n",
    "    variancias[grupo]   = variancias_series_temp\n",
    "    tamanhos[grupo]     = tamanhos_series_temp\n",
    "    t_inicio[grupo]     = t_inicio_series_temp\n",
    "    t_final[grupo]      = t_final_series_temp\n",
    "    delta_t[grupo]      = delta_t_series_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Técnica em que se analisa a tendência de uma séria a partir da variação da média ao longo do tempo. Consiste em mover uma janela de tamanho $n$ ao longo da série, calculando a média dos valores dentro da janela, construindo uma segunda série, a variação da média.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise das séries de batimentos cardíacos, com a finalidade de identificar automaticamente as fases do sono, a partir do algoritmo de segmentação, tem sido feita principalmente considerando que os segmentos (regiões de estacionariedade) correspondem de algum modo às fases, e essa correspondência pode ser exposta identificando diferenças das estatísticas entre os segmentos classificados e aglomerados segundo suas fases (a classificação é feita por frequência absoluta).\n",
    "\n",
    "Até então, não foi verificada uma diferença significativa entre os segmentos que permita a identificação das fases. Posso seguir buscando outras maneiras de reduzir cada segmento a um número e comparar as diferenças entre as fases. Esses valores seriam utilizados em algum algoritmo de classificação. Ou posso buscar uma nova maneira de utilizar o algoritmo de segmentação para resolver o problema. Um meio é o de verificar diferenças de tendências entre as fases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo e plot das tendências - moving averages\n",
    "\n",
    "\n",
    "# Definição da função para gerar as cores de fundo\n",
    "def axvlines(xs, c, max_val, ax=None, **plot_kwargs):\n",
    "    \"\"\"\n",
    "    Draw vertical lines on plot\n",
    "    :param xs: A scalar, list, or 1D array of horizontal offsets\n",
    "    :param ax: The axis (or none to use gca)\n",
    "    :param plot_kwargs: Keyword arguments to be passed to plot\n",
    "    :return: The plot object corresponding to the lines.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xs = np.array((xs, ) if np.isscalar(xs) else xs, copy=False)\n",
    "    lims = ax.get_ylim()\n",
    "    x_points = np.repeat(xs[:, None], repeats=3, axis=1).flatten()\n",
    "    y_points = np.repeat(np.array(lims + (np.nan, ))[None, :], repeats=len(xs), axis=0).flatten()\n",
    "    plot = ax.plot(x_points, max_val*y_points, scaley = False, **plot_kwargs, color = c)\n",
    "    return plot\n",
    "\n",
    "\n",
    "colors  = [\"gold\", \"lavender\", \"lightsteelblue\", \"thistle\", \"orchid\", \"lightcoral\"]\n",
    "t_series = [\"Serie 0\", \"Serie 1\", \"Serie 2\", \"Serie 3\"]\n",
    "t_grupo = [\"Hypertensive\", \"Normotensive\", \"Proband\"]\n",
    "\n",
    "\n",
    "\n",
    "for grupo in range(3):\n",
    "    for indiv in range(len(data[grupo])):\n",
    "        # Figura para plot das séries\n",
    "        fig, axs = plt.subplots(nrows = 4, ncols = 1)\n",
    "        fig.set_size_inches(40, 60)\n",
    "        for serie_id in range(4):\n",
    "            df = data[grupo][indiv]\n",
    "            serie = df[t_series[serie_id]]\n",
    "            tempo = df[\"Tempo\"]\n",
    "            mov_avg = serie.rolling(window=int(len(serie)/100)).mean()\n",
    "            mov_avg.fillna(mov_avg[mov_avg.index[1]])\n",
    "\n",
    "\n",
    "            max_val = mov_avg.max()\n",
    "            min_val = mov_avg.min()\n",
    "            for i in [0, 1, 2, 3, 4, 5]:\n",
    "                axvlines(df[df[\"SleepStage\"] == float(i)][\"Tempo\"].values, colors[i], max_val, ax = axs[serie_id])\n",
    "\n",
    "            axs[serie_id].plot(tempo, mov_avg, color = 'black')\n",
    "\n",
    "            axs[serie_id].set_xticks(range(0, int(tempo[len(tempo)-1]), 5000))\n",
    "            axs[serie_id].set_ylim(int(min_val) - 10, int(max_val))\n",
    "            axs[serie_id].set_xlim(tempo[0], tempo[len(tempo)-1])\n",
    "            axs[serie_id].set_xlabel(\"Time\")\n",
    "            axs[serie_id].set_xlabel(\"Average\")\n",
    "            axs[serie_id].set_title(t_series[serie_id]+\" - Moving Averages\")\n",
    "            \n",
    "            \n",
    "        fig.savefig(\"Plots/Moving Averages/\"+t_grupo[grupo]+\"/Individuo \"+str(indiv)+\".png\")     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva ROC 3 Fases - Eficiência de classificação via verificação direta do tamanho do segmento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = []\n",
    "tamanho = []\n",
    "fase = []\n",
    "todos = [[], [], []]\n",
    "series = 3\n",
    "\n",
    "# 1: pegar os valores pertinentes de todos os individuos, separados por grupos.\n",
    "for j in range(3):\n",
    "    for i in range(len(data[j])):\n",
    "        media = media + list(seg_res[j][i][series][\"mean\"].values)\n",
    "        tamanho = tamanho + list( seg_res[j][i][series][\"finish\"] - seg_res[j][i][series][\"start\"] )\n",
    "        fase = fase + list(seg_res[j][i][series][\"SleepStage\"])\n",
    "    todos[j] = pd.DataFrame(columns = [\"media\", \"tamanho\", \"fase ref\", \"prev\"])\n",
    "    todos[j][\"media\"] = media; todos[j][\"tamanho\"] = np.log(np.asarray(tamanho)); todos[j][\"fase ref\"] = fase\n",
    "    \n",
    "# 2: refaz a classificação dos segmentos por 3 fases.    \n",
    "for i in range(3):\n",
    "    for j in range(len(todos[i])):\n",
    "        if (todos[i][\"fase ref\"][j] == 1.0):\n",
    "            todos[i].loc[j, \"fase ref\"] = 0.0\n",
    "        elif (todos[i][\"fase ref\"][j] == 2.0 or todos[i][\"fase ref\"][j] == 3.0):\n",
    "            todos[i].loc[j, \"fase ref\"] = 1.0\n",
    "        elif (todos[i][\"fase ref\"][j] == 5.0):\n",
    "            todos[i].loc[j, \"fase ref\"] = 2.0\n",
    "\n",
    "# 3: separo as tabelas obtidas anteriormente pelas fases.            \n",
    "separacao_fases_ht = [[], [], []]\n",
    "separacao_fases_nt = [[], [], []]\n",
    "separacao_fases_pb = [[], [], []]\n",
    "for i in range(3):\n",
    "    separacao_fases_ht[i] = todos[0][(todos[0][\"fase ref\"] == i)]\n",
    "    separacao_fases_nt[i] = todos[1][(todos[1][\"fase ref\"] == i)]\n",
    "    separacao_fases_pb[i] = todos[2][(todos[2][\"fase ref\"] == i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médias dos tamanhos para cada fase\n",
    "\n",
    "# serie 0\n",
    "#ht_mean = np.array([3.85, 3.90, 4.14])\n",
    "#nt_mean = np.array([3.83, 3.79, 4.18])\n",
    "#pb_mean = np.array([3.73, 3.81, 3.95])\n",
    "\n",
    "# serie 1\n",
    "#ht_mean = np.array([3.91, 4.04, 4.32])\n",
    "#nt_mean = np.array([3.93, 3.95, 4.32])\n",
    "#pb_mean = np.array([3.82, 3.81, 4.05])\n",
    "\n",
    "# serie 2\n",
    "#ht_mean = np.array([3.77, 3.87, 4.29])\n",
    "#nt_mean = np.array([3.84, 4.02, 4.72])\n",
    "#pb_mean = np.array([3.89, 4.06, 4.45])\n",
    "\n",
    "# serie 3\n",
    "ht_mean = np.array([3.72, 3.87, 4.22])\n",
    "nt_mean = np.array([3.84, 3.99, 4.64])\n",
    "pb_mean = np.array([3.89, 4.06, 4.45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: faço uma previsão por comparação das diferenças absolutas do tamanho com o tamanho médio.\n",
    "@jit\n",
    "def prev(ht_mean, nt_mean, pb_mean, separacao_fases_ht, separacao_fases_nt, separacao_fases_pb):\n",
    "    for j in range(3):\n",
    "        for i in separacao_fases_ht[j].index:\n",
    "            separacao_fases_ht[j].loc[i,\"prev\"] = np.argmin(np.abs(ht_mean - separacao_fases_ht[j][\"tamanho\"].loc[i]))\n",
    "        for i in separacao_fases_nt[j].index:\n",
    "            separacao_fases_nt[j].loc[i,\"prev\"] = np.argmin(np.abs(nt_mean - separacao_fases_nt[j][\"tamanho\"].loc[i]))\n",
    "        for i in separacao_fases_pb[j].index:\n",
    "            separacao_fases_pb[j].loc[i,\"prev\"] = np.argmin(np.abs(pb_mean - separacao_fases_pb[j][\"tamanho\"].loc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev(ht_mean, nt_mean, pb_mean, separacao_fases_ht, separacao_fases_nt, separacao_fases_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: calculos os pontos para a ROC, avaliando a eficiência da previsão para cada fase, para cada grupo        \n",
    "points_ht = [[], []]\n",
    "points_ht[0] = [1 - (len(separacao_fases_ht[1][separacao_fases_ht[1][\"prev\"] == 1]) + len(separacao_fases_ht[2][separacao_fases_ht[2][\"prev\"] == 2]))/( len(separacao_fases_ht[1]) + len(separacao_fases_ht[2]) ),\n",
    "                1 - (len(separacao_fases_ht[0][separacao_fases_ht[0][\"prev\"] == 0]) + len(separacao_fases_ht[2][separacao_fases_ht[2][\"prev\"] == 2]))/( len(separacao_fases_ht[0]) + len(separacao_fases_ht[2]) ),\n",
    "                1 - (len(separacao_fases_ht[1][separacao_fases_ht[1][\"prev\"] == 1]) + len(separacao_fases_ht[0][separacao_fases_ht[0][\"prev\"] == 0]))/( len(separacao_fases_ht[0]) + len(separacao_fases_ht[1]) )]\n",
    "points_ht[1] = [len(separacao_fases_ht[0][separacao_fases_ht[0][\"prev\"] == 0])/len(separacao_fases_ht[0]),\n",
    "                len(separacao_fases_ht[1][separacao_fases_ht[1][\"prev\"] == 1])/len(separacao_fases_ht[1]),\n",
    "                len(separacao_fases_ht[2][separacao_fases_ht[2][\"prev\"] == 2])/len(separacao_fases_ht[2])]\n",
    "\n",
    "points_nt = [[], []]\n",
    "points_nt[0] = [1 - (len(separacao_fases_nt[1][separacao_fases_nt[1][\"prev\"] == 1]) + len(separacao_fases_nt[2][separacao_fases_nt[2][\"prev\"] == 2]))/( len(separacao_fases_nt[1]) + len(separacao_fases_nt[2]) ),\n",
    "                1 - (len(separacao_fases_nt[0][separacao_fases_nt[0][\"prev\"] == 0]) + len(separacao_fases_nt[2][separacao_fases_nt[2][\"prev\"] == 2]))/( len(separacao_fases_nt[0]) + len(separacao_fases_nt[2]) ),\n",
    "                1 - (len(separacao_fases_nt[1][separacao_fases_nt[1][\"prev\"] == 1]) + len(separacao_fases_nt[0][separacao_fases_nt[0][\"prev\"] == 0]))/( len(separacao_fases_nt[0]) + len(separacao_fases_nt[1]) )]\n",
    "points_nt[1] = [len(separacao_fases_nt[0][separacao_fases_nt[0][\"prev\"] == 0])/len(separacao_fases_nt[0]),\n",
    "                len(separacao_fases_nt[1][separacao_fases_nt[1][\"prev\"] == 1])/len(separacao_fases_nt[1]),\n",
    "                len(separacao_fases_nt[2][separacao_fases_nt[2][\"prev\"] == 2])/len(separacao_fases_nt[2])]\n",
    "\n",
    "points_pb = [[], []]\n",
    "points_pb[0] = [1 - (len(separacao_fases_pb[0][separacao_fases_pb[0][\"prev\"] == 0]) + len(separacao_fases_pb[2][separacao_fases_pb[2][\"prev\"] == 2]))/( len(separacao_fases_pb[0]) + len(separacao_fases_pb[2]) ),\n",
    "                1 - (len(separacao_fases_pb[1][separacao_fases_pb[1][\"prev\"] == 1]) + len(separacao_fases_pb[0][separacao_fases_pb[0][\"prev\"] == 0]))/( len(separacao_fases_pb[0]) + len(separacao_fases_pb[1]) ),\n",
    "                1 - (len(separacao_fases_pb[1][separacao_fases_pb[1][\"prev\"] == 1]) + len(separacao_fases_pb[2][separacao_fases_pb[2][\"prev\"] == 2]))/( len(separacao_fases_pb[1]) + len(separacao_fases_pb[2]) )]\n",
    "points_pb[1] = [len(separacao_fases_pb[0][separacao_fases_pb[0][\"prev\"] == 0])/len(separacao_fases_pb[0]),\n",
    "                len(separacao_fases_pb[1][separacao_fases_pb[1][\"prev\"] == 1])/len(separacao_fases_pb[1]),\n",
    "                len(separacao_fases_pb[2][separacao_fases_pb[2][\"prev\"] == 2])/len(separacao_fases_pb[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8, 8)\n",
    "fig.suptitle(\"ROC - Seg. Length - Series 3\")\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.set_xlabel(\"1 - Specificity\"); ax1.set_ylabel(\"Sensitivity\")\n",
    "ax1.set_xlim(0,1); ax1.set_ylim(0, 1)\n",
    "\n",
    "markers = ['o', 'x', 'D']\n",
    "labels  = [\"0\", \"1\", \"\"]\n",
    "\n",
    "ax1.plot(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), 'k--')\n",
    "for i in range(3):\n",
    "    ax1.scatter(points_ht[0][i], points_ht[1][i], marker = markers[i], color = 'k', label = labels[i]+\" - Hypertensive\")\n",
    "    ax1.scatter(points_nt[0][i], points_nt[1][i], marker = markers[i], color = 'b', label = labels[i]+\" - Normotensive\")\n",
    "    ax1.scatter(points_pb[0][i], points_pb[1][i], marker = markers[i], color = 'r', label = labels[i]+\" - Proband\")\n",
    "    \n",
    "ax1.legend()    \n",
    "fig.savefig(\"Plots/Medidas/3 Grupos/roc_serie3_tamanho.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva ROC 5 Fases - Eficiência de classificação via verificação direta do tamanho do segmento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = []\n",
    "tamanho = []\n",
    "fase = []\n",
    "todos = [[], [], []]\n",
    "series = 3\n",
    "\n",
    "# 1: pegar os valores pertinentes de todos os individuos, separados por grupos.\n",
    "for j in range(3):\n",
    "    for i in range(len(data[j])):\n",
    "        media = media + list(seg_res[j][i][series][\"mean\"].values)\n",
    "        tamanho = tamanho + list( seg_res[j][i][series][\"finish\"] - seg_res[j][i][series][\"start\"] )\n",
    "        fase = fase + list(seg_res[j][i][series][\"SleepStage\"])\n",
    "    todos[j] = pd.DataFrame(columns = [\"media\", \"tamanho\", \"fase ref\", \"prev\"])\n",
    "    todos[j][\"media\"] = media; todos[j][\"tamanho\"] = np.log(np.asarray(tamanho)); todos[j][\"fase ref\"] = fase\n",
    "    \n",
    "# 2: adapta a fase 5 para fase 4    \n",
    "for i in range(3):\n",
    "    for j in range(len(todos[i])):\n",
    "        if (todos[i][\"fase ref\"][j] == 5.0):\n",
    "            todos[i].loc[j, \"fase ref\"] = 4.0\n",
    "\n",
    "# 3: separo as tabelas obtidas anteriormente pelas fases.            \n",
    "separacao_fases_ht = [[], [], [], [], []]\n",
    "separacao_fases_nt = [[], [], [], [], []]\n",
    "separacao_fases_pb = [[], [], [], [], []]\n",
    "for i in range(5):\n",
    "    separacao_fases_ht[i] = todos[0][(todos[0][\"fase ref\"] == i)]\n",
    "    separacao_fases_nt[i] = todos[1][(todos[1][\"fase ref\"] == i)]\n",
    "    separacao_fases_pb[i] = todos[2][(todos[2][\"fase ref\"] == i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serie 0\n",
    "#ht_mean = np.array([3.85, 3.90, 4.14, 4.24, 3.82])\n",
    "#nt_mean = np.array([3.83, 3.79, 4.18, 4.22, 3.80])\n",
    "#pb_mean = np.array([3.73, 3.81, 3.95, 3.94, 3.80])\n",
    "\n",
    "# serie 1\n",
    "#ht_mean = np.array([3.91, 4.04, 4.32, 4.40, 3.84])\n",
    "#nt_mean = np.array([3.93, 3.95, 4.32, 4.24, 3.88])\n",
    "#pb_mean = np.array([3.82, 3.81, 4.05, 4.10, 3.90])\n",
    "\n",
    "# serie 2\n",
    "#ht_mean = np.array([3.77, 3.87, 4.29, 4.48, 3.76])\n",
    "#nt_mean = np.array([3.84, 4.02, 4.72, 4.82, 3.97])\n",
    "#pb_mean = np.array([3.89, 4.06, 4.45, 4.65, 3.89])\n",
    "\n",
    "# serie 3\n",
    "ht_mean = np.array([3.72, 3.87, 4.22, 4.40, 3.72])\n",
    "nt_mean = np.array([3.84, 3.99, 4.64, 4.67, 3.94])\n",
    "pb_mean = np.array([3.89, 4.06, 4.45, 4.65, 3.89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: faço uma previsão por comparação das diferenças absolutas do tamanho com o tamanho médio.\n",
    "@jit\n",
    "def prev(ht_mean, nt_mean, pb_mean, separacao_fases_ht, separacao_fases_nt, separacao_fases_pb):\n",
    "    for j in range(5):\n",
    "        for i in separacao_fases_ht[j].index:\n",
    "            separacao_fases_ht[j].loc[i,\"prev\"] = np.argmin(np.abs(ht_mean - separacao_fases_ht[j][\"tamanho\"].loc[i]))\n",
    "        for i in separacao_fases_nt[j].index:\n",
    "            separacao_fases_nt[j].loc[i,\"prev\"] = np.argmin(np.abs(nt_mean - separacao_fases_nt[j][\"tamanho\"].loc[i]))\n",
    "        for i in separacao_fases_pb[j].index:\n",
    "            separacao_fases_pb[j].loc[i,\"prev\"] = np.argmin(np.abs(pb_mean - separacao_fases_pb[j][\"tamanho\"].loc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev(ht_mean, nt_mean, pb_mean, separacao_fases_ht, separacao_fases_nt, separacao_fases_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: calculos os pontos para a ROC, avaliando a eficiência da previsão para cada fase, para cada grupo        \n",
    "points_ht = [[], []]\n",
    "points_ht[0] = [1 - (len(separacao_fases_ht[1][separacao_fases_ht[1][\"prev\"] == 1]) + len(separacao_fases_ht[2][separacao_fases_ht[2][\"prev\"] == 2]) + len(separacao_fases_ht[3][separacao_fases_ht[3][\"prev\"] == 3]) + len(separacao_fases_ht[4][separacao_fases_ht[4][\"prev\"] == 4]))/( len(separacao_fases_ht[1]) + len(separacao_fases_ht[2]) + len(separacao_fases_ht[3]) + len(separacao_fases_ht[4]) ),\n",
    "                1 - (len(separacao_fases_ht[0][separacao_fases_ht[0][\"prev\"] == 0]) + len(separacao_fases_ht[2][separacao_fases_ht[2][\"prev\"] == 2]) + len(separacao_fases_ht[3][separacao_fases_ht[3][\"prev\"] == 3]) + len(separacao_fases_ht[4][separacao_fases_ht[4][\"prev\"] == 4]))/( len(separacao_fases_ht[0]) + len(separacao_fases_ht[2]) + len(separacao_fases_ht[3]) + len(separacao_fases_ht[4]) ),\n",
    "                1 - (len(separacao_fases_ht[1][separacao_fases_ht[1][\"prev\"] == 1]) + len(separacao_fases_ht[0][separacao_fases_ht[0][\"prev\"] == 0]) + len(separacao_fases_ht[3][separacao_fases_ht[3][\"prev\"] == 3]) + len(separacao_fases_ht[4][separacao_fases_ht[4][\"prev\"] == 4]))/( len(separacao_fases_ht[1]) + len(separacao_fases_ht[0]) + len(separacao_fases_ht[3]) + len(separacao_fases_ht[4]) ),\n",
    "                1 - (len(separacao_fases_ht[1][separacao_fases_ht[1][\"prev\"] == 1]) + len(separacao_fases_ht[2][separacao_fases_ht[2][\"prev\"] == 2]) + len(separacao_fases_ht[0][separacao_fases_ht[0][\"prev\"] == 0]) + len(separacao_fases_ht[4][separacao_fases_ht[4][\"prev\"] == 4]))/( len(separacao_fases_ht[1]) + len(separacao_fases_ht[2]) + len(separacao_fases_ht[0]) + len(separacao_fases_ht[4]) ),\n",
    "                1 - (len(separacao_fases_ht[1][separacao_fases_ht[1][\"prev\"] == 1]) + len(separacao_fases_ht[2][separacao_fases_ht[2][\"prev\"] == 2]) + len(separacao_fases_ht[3][separacao_fases_ht[3][\"prev\"] == 3]) + len(separacao_fases_ht[0][separacao_fases_ht[0][\"prev\"] == 0]))/( len(separacao_fases_ht[1]) + len(separacao_fases_ht[2]) + len(separacao_fases_ht[3]) + len(separacao_fases_ht[0]) )]\n",
    "points_ht[1] = [len(separacao_fases_ht[0][separacao_fases_ht[0][\"prev\"] == 0])/len(separacao_fases_ht[0]),\n",
    "                len(separacao_fases_ht[1][separacao_fases_ht[1][\"prev\"] == 1])/len(separacao_fases_ht[1]),\n",
    "                len(separacao_fases_ht[2][separacao_fases_ht[2][\"prev\"] == 2])/len(separacao_fases_ht[2]),\n",
    "                len(separacao_fases_ht[3][separacao_fases_ht[3][\"prev\"] == 3])/len(separacao_fases_ht[3]),\n",
    "                len(separacao_fases_ht[4][separacao_fases_ht[4][\"prev\"] == 4])/len(separacao_fases_ht[4])]\n",
    "\n",
    "points_nt = [[], []]\n",
    "points_nt[0] = [1 - (len(separacao_fases_nt[1][separacao_fases_nt[1][\"prev\"] == 1]) + len(separacao_fases_nt[2][separacao_fases_nt[2][\"prev\"] == 2]) + len(separacao_fases_nt[3][separacao_fases_nt[3][\"prev\"] == 3]) + len(separacao_fases_nt[4][separacao_fases_nt[4][\"prev\"] == 4]))/( len(separacao_fases_nt[1]) + len(separacao_fases_nt[2]) + len(separacao_fases_nt[3]) + len(separacao_fases_nt[4]) ),\n",
    "                1 - (len(separacao_fases_nt[0][separacao_fases_nt[0][\"prev\"] == 0]) + len(separacao_fases_nt[2][separacao_fases_nt[2][\"prev\"] == 2]) + len(separacao_fases_nt[3][separacao_fases_nt[3][\"prev\"] == 3]) + len(separacao_fases_nt[4][separacao_fases_nt[4][\"prev\"] == 4]))/( len(separacao_fases_nt[0]) + len(separacao_fases_nt[2]) + len(separacao_fases_nt[3]) + len(separacao_fases_nt[4]) ),\n",
    "                1 - (len(separacao_fases_nt[1][separacao_fases_nt[1][\"prev\"] == 1]) + len(separacao_fases_nt[0][separacao_fases_nt[0][\"prev\"] == 0]) + len(separacao_fases_nt[3][separacao_fases_nt[3][\"prev\"] == 3]) + len(separacao_fases_nt[4][separacao_fases_nt[4][\"prev\"] == 4]))/( len(separacao_fases_nt[1]) + len(separacao_fases_nt[0]) + len(separacao_fases_nt[3]) + len(separacao_fases_nt[4]) ),\n",
    "                1 - (len(separacao_fases_nt[1][separacao_fases_nt[1][\"prev\"] == 1]) + len(separacao_fases_nt[2][separacao_fases_nt[2][\"prev\"] == 2]) + len(separacao_fases_nt[0][separacao_fases_nt[0][\"prev\"] == 0]) + len(separacao_fases_nt[4][separacao_fases_nt[4][\"prev\"] == 4]))/( len(separacao_fases_nt[1]) + len(separacao_fases_nt[2]) + len(separacao_fases_nt[0]) + len(separacao_fases_nt[4]) ),\n",
    "                1 - (len(separacao_fases_nt[1][separacao_fases_nt[1][\"prev\"] == 1]) + len(separacao_fases_nt[2][separacao_fases_nt[2][\"prev\"] == 2]) + len(separacao_fases_nt[3][separacao_fases_nt[3][\"prev\"] == 3]) + len(separacao_fases_nt[0][separacao_fases_nt[0][\"prev\"] == 0]))/( len(separacao_fases_nt[1]) + len(separacao_fases_nt[2]) + len(separacao_fases_nt[3]) + len(separacao_fases_nt[0]) )]\n",
    "points_nt[1] = [len(separacao_fases_nt[0][separacao_fases_nt[0][\"prev\"] == 0])/len(separacao_fases_nt[0]),\n",
    "                len(separacao_fases_nt[1][separacao_fases_nt[1][\"prev\"] == 1])/len(separacao_fases_nt[1]),\n",
    "                len(separacao_fases_nt[2][separacao_fases_nt[2][\"prev\"] == 2])/len(separacao_fases_nt[2]),\n",
    "                len(separacao_fases_nt[3][separacao_fases_nt[3][\"prev\"] == 3])/len(separacao_fases_nt[3]),\n",
    "                len(separacao_fases_nt[4][separacao_fases_nt[4][\"prev\"] == 4])/len(separacao_fases_nt[4])]\n",
    "\n",
    "points_pb = [[], []]\n",
    "points_pb[0] = [1 - (len(separacao_fases_pb[1][separacao_fases_pb[1][\"prev\"] == 1]) + len(separacao_fases_pb[2][separacao_fases_pb[2][\"prev\"] == 2]) + len(separacao_fases_pb[3][separacao_fases_pb[3][\"prev\"] == 3]) + len(separacao_fases_pb[4][separacao_fases_pb[4][\"prev\"] == 4]))/( len(separacao_fases_pb[1]) + len(separacao_fases_pb[2]) + len(separacao_fases_pb[3]) + len(separacao_fases_pb[4]) ),\n",
    "                1 - (len(separacao_fases_pb[0][separacao_fases_pb[0][\"prev\"] == 0]) + len(separacao_fases_pb[2][separacao_fases_pb[2][\"prev\"] == 2]) + len(separacao_fases_pb[3][separacao_fases_pb[3][\"prev\"] == 3]) + len(separacao_fases_pb[4][separacao_fases_pb[4][\"prev\"] == 4]))/( len(separacao_fases_pb[0]) + len(separacao_fases_pb[2]) + len(separacao_fases_pb[3]) + len(separacao_fases_pb[4]) ),\n",
    "                1 - (len(separacao_fases_pb[1][separacao_fases_pb[1][\"prev\"] == 1]) + len(separacao_fases_pb[0][separacao_fases_pb[0][\"prev\"] == 0]) + len(separacao_fases_pb[3][separacao_fases_pb[3][\"prev\"] == 3]) + len(separacao_fases_pb[4][separacao_fases_pb[4][\"prev\"] == 4]))/( len(separacao_fases_pb[1]) + len(separacao_fases_pb[0]) + len(separacao_fases_pb[3]) + len(separacao_fases_pb[4]) ),\n",
    "                1 - (len(separacao_fases_pb[1][separacao_fases_pb[1][\"prev\"] == 1]) + len(separacao_fases_pb[2][separacao_fases_pb[2][\"prev\"] == 2]) + len(separacao_fases_pb[0][separacao_fases_pb[0][\"prev\"] == 0]) + len(separacao_fases_pb[4][separacao_fases_pb[4][\"prev\"] == 4]))/( len(separacao_fases_pb[1]) + len(separacao_fases_pb[2]) + len(separacao_fases_pb[0]) + len(separacao_fases_pb[4]) ),\n",
    "                1 - (len(separacao_fases_pb[1][separacao_fases_pb[1][\"prev\"] == 1]) + len(separacao_fases_pb[2][separacao_fases_pb[2][\"prev\"] == 2]) + len(separacao_fases_pb[3][separacao_fases_pb[3][\"prev\"] == 3]) + len(separacao_fases_pb[0][separacao_fases_pb[0][\"prev\"] == 0]))/( len(separacao_fases_pb[1]) + len(separacao_fases_pb[2]) + len(separacao_fases_pb[3]) + len(separacao_fases_pb[0]) )]\n",
    "points_pb[1] = [len(separacao_fases_pb[0][separacao_fases_pb[0][\"prev\"] == 0])/len(separacao_fases_pb[0]),\n",
    "                len(separacao_fases_pb[1][separacao_fases_pb[1][\"prev\"] == 1])/len(separacao_fases_pb[1]),\n",
    "                len(separacao_fases_pb[2][separacao_fases_pb[2][\"prev\"] == 2])/len(separacao_fases_pb[2]),\n",
    "                len(separacao_fases_pb[3][separacao_fases_pb[3][\"prev\"] == 3])/len(separacao_fases_pb[3]),\n",
    "                len(separacao_fases_pb[4][separacao_fases_pb[4][\"prev\"] == 4])/len(separacao_fases_pb[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "fig.set_size_inches(8, 8)\n",
    "fig.suptitle(\"ROC - Seg. Length - Series 3\")\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.set_xlabel(\"1 - Specificity\"); ax1.set_ylabel(\"Sensitivity\")\n",
    "ax1.set_xlim(0,1); ax1.set_ylim(0, 1)\n",
    "\n",
    "markers = ['o', 'x', 'D', 's', 'P']\n",
    "labels  = ['0', '1', '2', '3', 'REM']\n",
    "\n",
    "ax1.plot(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1), 'k--')\n",
    "for i in range(5):\n",
    "    ax1.scatter(points_ht[0][i], points_ht[1][i], marker = markers[i], color = 'k', label = labels[i]+\" - Hypertensive\")\n",
    "    ax1.scatter(points_nt[0][i], points_nt[1][i], marker = markers[i], color = 'b', label = labels[i]+\" - Normotensive\")\n",
    "    ax1.scatter(points_pb[0][i], points_pb[1][i], marker = markers[i], color = 'r', label = labels[i]+\" - Proband\")\n",
    "\n",
    "ax1.legend()\n",
    "fig.savefig(\"Plots/Medidas/5 Grupos/roc_serie3_tamanho.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
